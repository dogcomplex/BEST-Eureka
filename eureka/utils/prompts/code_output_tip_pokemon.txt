The output of the reward function should consist of two items:
    (1) the total reward,
    (2) a dictionary of each individual reward component.
The code output should be formatted as a python code string: "```python ... ```".

Some helpful tips for writing the reward function code:
    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations to the overall reward or its components
    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable
    (3) Make sure the type of each input variable is correctly specified; a float input variable should not be specified as torch.Tensor
    (4) Most importantly, the reward code's input variables must contain only attributes of the provided environment class definition (namely, variables that have prefix self.). Under no circumstance can you introduce new input variables.


REQUIREMENTS:
(1) You may only modify the compute_success() function.  All other proposed code changes will be discarded.  
(2) The rest of the code is read-only, shown as references callable from compute_success()
(3) You may not use references to 'self' within compute_success(), but can assume any class variable will be available via function variables.  These are generally floats NOT tensors, but check the rest of the class code
(4) However, you may define sub-functions within compute_success() which replace the functionality of other functions, as long as the sub-functions are called from compute_success()
(5) You may use any addressable memory code known from the Pokemon Red game, but try to stick to the spirit of the game (no rom hacking)
(6) The goal is to play the game like a realistic human player, or better, progressing through the story, catching as many unique pokemon as possible, earning badges and beating the elite four
(7) Try to keep code size compact and generalized still.  You have a big context limit but it's still there
(8) Be careful not to define input variables as torch.Tensor when the class variable is actually a float
(9) Don't use torch.exp() on floats!

STRATEGY:
(1) Most importantly, remember this is a process.  We want to make small improvements, or learn more with every iteration.  
(2) Every sub_reward output from compute_success() will be tracked over time during the game, with the list of scores, mean, min and max provided to the next iteration of reward function improvement.  Tracking useful data is a good way to do this!
(3) Additional functions or data can be built within compute_success() if needed, e.g. to lookup additional memory address codes.
(4) Rewards that don't seem to change may just be aspects of the game that were not encountered yet during gameplay.  Consider improving other aspects first to try and push for enough progress to get to those aspects. (e.g. advancements in exploration are needed before being able to challenge a gym).  Ignore rewards that never change and try to normalize their score to to 0 where possible in the meantime.
(5) To be the very best, you will have to do more than just tune existing reward variables.  You'll need to recognize different modes of the game and optimize them with different strategies.  Exploration takes different skills than pokemon battling!
(6)  If further meaningful progress can not be achieved without changes to functions outside of compute_success(), describe the proposed changes in your report.